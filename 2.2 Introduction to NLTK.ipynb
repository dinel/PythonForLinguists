{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is NLTK\n",
    "\n",
    "Natural Language Processing is often taught within the confines of a single-semester course at advanced undergraduate level or postgraduate level. Many instructors have found that it is difficult to cover both the theoretical and practical sides of the subject in such a short span of time. Some courses focus on theory to the exclusion of practical exercises, and deprive students of the challenge and excitement of writing programs to automatically process language. Other courses are simply designed to teach programming for linguists, and do not manage to cover any significant NLP content. NLTK was originally developed to address this problem, making it feasible to cover a substantial amount of theory and practice within a single-semester course, even if students have no prior programming experience.\n",
    "\n",
    "From <a href=\"http://www.nltk.org/book/ch00.html\" target=\"_blank\">http://www.nltk.org/book/ch00.html</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is NLTK\n",
    "\n",
    "- a collection of language processing algorithms implemented in python\n",
    "- many of the algorithms are baseline implementation with average/low accuracy\n",
    "- a collection of corpora and resources in various languages processed so they can be easily accessed\n",
    "- Excellent way of accessing packages written in other languages\n",
    "\n",
    "- **My view**: great for learning programming and language processing, but largely over-rated/misinterpreted as a \"production ready\" toolkit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Installing NLTK\n",
    "\n",
    "In order to install NLTK you will need python3 (the recommended version is **python 3.5 32bit version**)\n",
    "\n",
    "Installation instructions for NLTK available at <a href=\"http://www.nltk.org/install.html\">http://www.nltk.org/install.html</a>\n",
    "\n",
    "To test if you have successfully installed NLTK type ``import nltk`` in the python shell. If no error is displayed it means NLTK is successfully installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Downloading the datasets\n",
    "\n",
    "NLTK comes with some great datasets ready to be used. You can download them using the following sequence of commands.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "If everything works correctly you will see the following dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://www.nltk.org/images/nltk-downloader.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://www.nltk.org/images/nltk-downloader.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Download \"book\" for a fairly large collection of datasets that are used in the NLTK book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to find out installed corpora\n",
    "\n",
    "Call ``nltk.download()`` and see which corpora are marked as installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'brown', 'brown.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'dependency_treebank', 'dependency_treebank.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'movie_reviews', 'movie_reviews.zip', 'names', 'names.zip', 'nps_chat', 'nps_chat.zip', 'panlex_swadesh.zip', 'ppattach', 'ppattach.zip', 'reuters.zip', 'senseval', 'senseval.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'swadesh', 'swadesh.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "# programatic way of finding yout the installed corpora\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "print( os.listdir( nltk.data.find(\"corpora\") ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Find out about a corpus\n",
    "\n",
    "Use ``nltk.corpora.<corpus name>.readme()`` to display the readme file associated with the corpus, if it exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names Corpus, Version 1.3 (1994-03-29)\n",
      "Copyright (C) 1991 Mark Kantrowitz\n",
      "Additions by Bill Ross\n",
      "\n",
      "This corpus contains 5001 female names and 2943 male names, sorted\n",
      "alphabetically, one per line.\n",
      "\n",
      "You may use the lists of names for any purpose, so long as credit is\n",
      "given in any published work. You may also redistribute the list if you\n",
      "provide the recipients with a copy of this README file. The lists are\n",
      "not in the public domain (I retain the copyright on the lists) but are\n",
      "freely redistributable.  If you have any additions to the lists of\n",
      "names, I would appreciate receiving them.\n",
      "\n",
      "Mark Kantrowitz <mkant+@cs.cmu.edu>\n",
      "http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.names.readme())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Getting the texts in the corpus\n",
    "\n",
    "``nltk.corpus.<corpus name>.fileids()`` returns a list of the files included in the corpus. Usually they correspond to individual texts. Usually the IDs returned can be used as parameters of functions which process texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Accessing the text \n",
    "\n",
    "- ``nltk.corpus.<corpus name>.raw(<fileID>)`` accesses the raw text. The result is a string\n",
    "- ``nltk.corpus.<corpus name>.words(<fileID>)`` returns a list of words from the text/corpus\n",
    "- ``nltk.corpus.<corpus name>.sents(<fileID>)`` returns a list of sentences from the text/corpus\n",
    "- ``nltk.corpus.<corpus name>.paras(<fileID>)`` returns a list of sentences from the text/corpus\n",
    "- ``nltk.corpus.<corpus name>.tagged_words(<fileID>)``, ``nltk.corpus.<corpus name>.tagged_sents(<fileID>)``, ``nltk.corpus.<corpus name>.tagged_paras(<fileID>)`` - similar to the above, but the words are tagged\n",
    "\n",
    "``<fileID>`` is an optional parameter and restricts the extraction of information only to the files specified. \n",
    "\n",
    "**Note**: not all the corpora contain this information. You need to try or use ``dir(nltk.corpus.<corpus name>)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_add', '_c2f', '_delimiter', '_encoding', '_f2c', '_file', '_fileids', '_get_root', '_init', '_map', '_para_block_reader', '_pattern', '_resolve', '_root', '_sent_tokenizer', '_sep', '_tagset', '_unload', '_word_tokenizer', 'abspath', 'abspaths', 'categories', 'citation', 'encoding', 'ensure_loaded', 'fileids', 'license', 'open', 'paras', 'raw', 'readme', 'root', 'sents', 'tagged_paras', 'tagged_sents', 'tagged_words', 'unicode_repr', 'words']\n"
     ]
    }
   ],
   "source": [
    "# NLTK employs a lazy loading mechanism, so unless the corpus is accessed \n",
    "# dir does not display the correct information. Hint the information is not\n",
    "# correct is reference to Lazy loading in the output of dir\n",
    "\n",
    "# display information about Brown corpus\n",
    "nltk.corpus.brown.readme()\n",
    "print(dir(nltk.corpus.brown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example from Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROWN CORPUS\n",
      "\n",
      "A Standard Corpus of Present-Day Edited American\n",
      "English, for use with Digital Computers.\n",
      "\n",
      "by W. N. Francis and H. Kucera (1964)\n",
      "Department of Linguistics, Brown University\n",
      "Providence, Rhode Island, USA\n",
      "\n",
      "Revised 1971, Revised and Amplified 1979\n",
      "\n",
      "http://www.hit.uib.no/icame/brown/bcm.html\n",
      "\n",
      "Distributed with the permission of the copyright holder,\n",
      "redistribution permitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.brown.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n"
     ]
    }
   ],
   "source": [
    "# get the list of texts in the corpus\n",
    "print(nltk.corpus.brown.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# access the categories of brown corpus\n",
    "print(nltk.corpus.brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/c\n"
     ]
    }
   ],
   "source": [
    "# access the raw corpus\n",
    "print(nltk.corpus.brown.raw(\"ca01\")[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "# access the words\n",
    "print(nltk.corpus.brown.words(\"ca01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n"
     ]
    }
   ],
   "source": [
    "# access the tagged words\n",
    "print(nltk.corpus.brown.tagged_words(\"ca01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "# access sentences\n",
    "print(nltk.corpus.brown.sents(categories=\"news\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "# access tagged sentences\n",
    "print(nltk.corpus.brown.tagged_sents(categories=\"news\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Producing frequency lists\n",
    "\n",
    "- frequency lists can be easily produced using ``FreqDist``. \n",
    "- FreqDist takes as a parameter a sequence that needs to be counted (i.e. it is not necessary words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# producing the frequency list from Brown corpus\n",
    "fdist1 = nltk.FreqDist(nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 62713), (',', 58334), ('.', 49346), ('of', 36080), ('and', 27915), ('to', 25732), ('a', 21881), ('in', 19536), ('that', 10237), ('is', 10011), ('was', 9777), ('for', 8841), ('``', 8837), (\"''\", 8789), ('The', 7258), ('with', 7012), ('it', 6723), ('as', 6706), ('he', 6566), ('his', 6466), ('on', 6395), ('be', 6344), (';', 5566), ('I', 5161), ('by', 5103), ('had', 5102), ('at', 4963), ('?', 4693), ('not', 4423), ('are', 4333), ('from', 4207), ('or', 4118), ('this', 3966), ('have', 3892), ('an', 3542), ('which', 3540), ('--', 3432), ('were', 3279), ('but', 3007), ('He', 2982), ('her', 2885), ('one', 2873), ('they', 2773), ('you', 2766), ('all', 2726), ('would', 2677), ('him', 2576), ('their', 2562), ('been', 2470), (')', 2466)]\n"
     ]
    }
   ],
   "source": [
    "# print the most common 50 words\n",
    "print(fdist1.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# use list comprehension to produce a frequency list of the words from \n",
    "# Brown corpus which are not stopwords \n",
    "fdist2 = nltk.FreqDist([word.lower() \n",
    "                        for word in nltk.corpus.brown.words() \n",
    "                        if word.isalnum()\n",
    "                        if word.lower() not in nltk.corpus.stopwords.words('english')\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 3292), ('would', 2714), ('said', 1961), ('new', 1635), ('could', 1601), ('time', 1598), ('two', 1412), ('may', 1402), ('first', 1361), ('like', 1292), ('man', 1207), ('even', 1170), ('made', 1125), ('also', 1069), ('many', 1030), ('must', 1013), ('af', 996), ('back', 966), ('years', 950), ('much', 937), ('way', 908), ('well', 897), ('people', 847), ('little', 831), ('state', 807), ('good', 806), ('make', 794), ('world', 787), ('still', 782), ('see', 772), ('men', 763), ('work', 762), ('long', 752), ('get', 749), ('life', 715), ('never', 697), ('day', 687), ('another', 684), ('know', 683), ('last', 676), ('us', 675), ('might', 672), ('great', 665), ('old', 661), ('year', 658), ('come', 630), ('since', 628), ('go', 626), ('came', 622), ('right', 613)]\n"
     ]
    }
   ],
   "source": [
    "# print the most common 50 words\n",
    "print(fdist2.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 3292), ('would', 2714), ('said', 1961), ('new', 1635), ('could', 1601), ('time', 1598), ('two', 1412), ('may', 1402), ('first', 1361), ('like', 1292), ('man', 1207), ('even', 1170), ('made', 1125), ('also', 1069), ('many', 1030), ('must', 1013), ('af', 996), ('back', 966), ('years', 950), ('much', 937), ('way', 908), ('well', 897), ('people', 847), ('little', 831), ('state', 807), ('good', 806), ('make', 794), ('world', 787), ('still', 782), ('see', 772), ('men', 763), ('work', 762), ('long', 752), ('get', 749), ('life', 715), ('never', 697), ('day', 687), ('another', 684), ('know', 683), ('last', 676), ('us', 675), ('might', 672), ('great', 665), ('old', 661), ('year', 658), ('come', 630), ('since', 628), ('go', 626), ('came', 622), ('right', 613)]\n"
     ]
    }
   ],
   "source": [
    "# doing the same using map and filter\n",
    "# convert everything to lower case\n",
    "lower_words = map(str.lower, nltk.corpus.brown.words())\n",
    "# filter punctuation\n",
    "no_punctuation = filter(str.isalnum, lower_words)\n",
    "# filter stopwords\n",
    "no_stopwords = filter(lambda x: x not in nltk.corpus.stopwords.words('english'), no_punctuation)\n",
    "fdist3 = nltk.FreqDist(no_stopwords)\n",
    "print(fdist3.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 3292), ('would', 2714), ('said', 1961), ('new', 1635), ('could', 1601), ('time', 1598), ('two', 1412), ('may', 1402), ('first', 1361), ('like', 1292), ('man', 1207), ('even', 1170), ('made', 1125), ('also', 1069), ('many', 1030), ('must', 1013), ('af', 996), ('back', 966), ('years', 950), ('much', 937), ('way', 908), ('well', 897), ('people', 847), ('little', 831), ('state', 807), ('good', 806), ('make', 794), ('world', 787), ('still', 782), ('see', 772), ('men', 763), ('work', 762), ('long', 752), ('get', 749), ('life', 715), ('never', 697), ('day', 687), ('another', 684), ('know', 683), ('last', 676), ('us', 675), ('might', 672), ('great', 665), ('old', 661), ('year', 658), ('come', 630), ('since', 628), ('go', 626), ('came', 622), ('right', 613)]\n"
     ]
    }
   ],
   "source": [
    "# of course we can put everything in one line\n",
    "fdist4 = nltk.FreqDist(\n",
    "    filter(lambda x: x not in nltk.corpus.stopwords.words('english'), \n",
    "           filter(str.isalnum, \n",
    "                  map(str.lower, nltk.corpus.brown.words())\n",
    "                 )\n",
    "          )\n",
    ")\n",
    "print(fdist4.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Producing ngrams\n",
    "\n",
    "- bigrams can be easily produced using ``nltk.bigrams(<list>)``, where <list> is a list of elements that will be used to produce bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Fulton'), ('Fulton', 'County'), ('County', 'Grand'), ('Grand', 'Jury'), ('Jury', 'said'), ('said', 'Friday'), ('Friday', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('of', \"Atlanta's\")]\n"
     ]
    }
   ],
   "source": [
    "bigrams_brown = nltk.bigrams(nltk.corpus.brown.words('ca01'))\n",
    "print(list(bigrams_brown)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('.', 'The'), 23), ((',', 'the'), 16), (('in', 'the'), 15), ((\"''\", '.'), 15), (('of', 'the'), 14), (('.', '``'), 8), ((\"''\", ','), 7), (('the', 'jury'), 7), (('jury', 'said'), 7), (('Fulton', 'County'), 6), (('The', 'jury'), 6), (('the', 'election'), 6), (('.', 'It'), 6), (('at', 'the'), 6), (('that', 'the'), 5), (('the', 'Fulton'), 5), ((',', 'and'), 5), ((',', '``'), 4), (('of', \"Georgia's\"), 4), (('to', 'the'), 4), (('should', 'be'), 4), (('the', 'State'), 4), (('said', '.'), 4), (('on', 'the'), 4), (('has', 'been'), 4), (('will', 'be'), 4), ((',', 'who'), 4), (('a', 'candidate'), 4), (('Highway', 'Department'), 4), (('.', 'A'), 4), (('the', 'House'), 4), ((',', 'which'), 3), (('election', ','), 3), (('for', 'the'), 3), (('which', 'the'), 3), (('said', ','), 3), (('number', 'of'), 3), (('and', 'the'), 3), (('said', 'it'), 3), (('``', 'are'), 3), (('the', 'Atlanta'), 3), (('as', 'a'), 3), (('in', 'a'), 3), (('added', 'that'), 3), (('race', ','), 3), ((',', 'a'), 3), (('there', 'was'), 3), (('to', 'make'), 3), (('make', 'the'), 3), (('is', 'expected'), 3)]\n"
     ]
    }
   ],
   "source": [
    "# the bigrams can be used by FreqDist\n",
    "fdist5 = nltk.FreqDist(nltk.bigrams(nltk.corpus.brown.words('ca01')))\n",
    "print(fdist5.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('.', '.'), ('The', 'AT')), 23), (((',', ','), ('the', 'AT')), 16), ((('in', 'IN'), ('the', 'AT')), 15), (((\"''\", \"''\"), ('.', '.')), 15), ((('of', 'IN'), ('the', 'AT')), 14), ((('.', '.'), ('``', '``')), 8), (((\"''\", \"''\"), (',', ',')), 7), ((('the', 'AT'), ('jury', 'NN')), 7), ((('jury', 'NN'), ('said', 'VBD')), 7), ((('Fulton', 'NP-TL'), ('County', 'NN-TL')), 6), ((('The', 'AT'), ('jury', 'NN')), 6), ((('the', 'AT'), ('election', 'NN')), 6), ((('.', '.'), ('It', 'PPS')), 6), ((('at', 'IN'), ('the', 'AT')), 6), ((('that', 'CS'), ('the', 'AT')), 5), (((',', ','), ('and', 'CC')), 5), (((',', ','), ('``', '``')), 4), ((('of', 'IN'), (\"Georgia's\", 'NP$')), 4), ((('to', 'IN'), ('the', 'AT')), 4), ((('should', 'MD'), ('be', 'BE')), 4), ((('the', 'AT'), ('State', 'NN-TL')), 4), ((('the', 'AT'), ('Fulton', 'NP-TL')), 4), ((('said', 'VBD'), ('.', '.')), 4), ((('on', 'IN'), ('the', 'AT')), 4), ((('has', 'HVZ'), ('been', 'BEN')), 4), ((('will', 'MD'), ('be', 'BE')), 4), (((',', ','), ('who', 'WPS')), 4), ((('a', 'AT'), ('candidate', 'NN')), 4), ((('Highway', 'NN-TL'), ('Department', 'NN-TL')), 4), ((('the', 'AT'), ('House', 'NN-TL')), 4), (((',', ','), ('which', 'WDT')), 3), ((('election', 'NN'), (',', ',')), 3), ((('for', 'IN'), ('the', 'AT')), 3), ((('which', 'WDT'), ('the', 'AT')), 3), ((('said', 'VBD'), (',', ',')), 3), ((('number', 'NN'), ('of', 'IN')), 3), ((('and', 'CC'), ('the', 'AT')), 3), ((('said', 'VBD'), ('it', 'PPS')), 3), ((('``', '``'), ('are', 'BER')), 3), ((('as', 'CS'), ('a', 'AT')), 3), ((('in', 'IN'), ('a', 'AT')), 3), ((('added', 'VBD'), ('that', 'CS')), 3), ((('race', 'NN'), (',', ',')), 3), (((',', ','), ('a', 'AT')), 3), ((('there', 'EX'), ('was', 'BEDZ')), 3), ((('to', 'TO'), ('make', 'VB')), 3), ((('make', 'VB'), ('the', 'AT')), 3), ((('is', 'BEZ'), ('expected', 'VBN')), 3), ((('expected', 'VBN'), ('to', 'TO')), 3), ((('to', 'TO'), ('issue', 'VB')), 3)]\n"
     ]
    }
   ],
   "source": [
    "# we can produce bigrams of other things\n",
    "fdist6 = nltk.FreqDist(nltk.bigrams(nltk.corpus.brown.tagged_words('ca01')))\n",
    "print(fdist6.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This list is quite difficult to read. As an exercise make it more readable by removing bigrams that contain punctuation and display the result a more friendly way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ngrams \n",
    "\n",
    "``nltk.ngrams(<list>, <n>)`` is used to produce ngrams from <list>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Fulton', 'County', 'Grand', 'Jury'), ('Fulton', 'County', 'Grand', 'Jury', 'said'), ('County', 'Grand', 'Jury', 'said', 'Friday'), ('Grand', 'Jury', 'said', 'Friday', 'an'), ('Jury', 'said', 'Friday', 'an', 'investigation'), ('said', 'Friday', 'an', 'investigation', 'of'), ('Friday', 'an', 'investigation', 'of', \"Atlanta's\"), ('an', 'investigation', 'of', \"Atlanta's\", 'recent'), ('investigation', 'of', \"Atlanta's\", 'recent', 'primary'), ('of', \"Atlanta's\", 'recent', 'primary', 'election')]\n"
     ]
    }
   ],
   "source": [
    "five_grams = nltk.ngrams(nltk.corpus.brown.words('ca01'), 5)\n",
    "print(list(five_grams)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "- Investigate the existing corpora. Find out one that you think may be useful for your research. Check what kind of information is available for the corpus. Produce various frequency lists from it. You will be asked to discuss your findings later on.\n",
    "- As the user for a word and find out whether it appears in the Brown corpus more frequently as a noun or as a verb. Let the user know if according to the tagging available for the Brown corpus the word is neither. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further reading\n",
    "\n",
    "Chapters 1 and 2 of NLTK book: <a href=\"http://www.nltk.org/book/ch01.html\" target=\"_blank\">http://www.nltk.org/book/ch01.html</a> and <a href=\"http://www.nltk.org/book/ch02.html\" target=\"_blank\">http://www.nltk.org/book/ch02.html</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Python programming for linguists</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://dinel.org.uk\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Constantin Orasan</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/dinel/PythonForLinguists\" rel=\"dct:source\">https://github.com/dinel/PythonForLinguists</a>."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
